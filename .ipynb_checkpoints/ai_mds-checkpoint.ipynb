{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <231012EE-C624-39BF-9168-0899EB683529> /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mTODO: Visualize which MDS are most likely to be mixed up\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/__init__.py:9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"LightGBM, Light Gradient Boosting Machine.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/microsoft/LightGBM/graphs/contributors.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopException, early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/basic.py:281\u001b[0m\n\u001b[1;32m    279\u001b[0m     _LIB \u001b[38;5;241m=\u001b[39m Mock(ctypes\u001b[38;5;241m.\u001b[39mCDLL)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m _NUMERIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/basic.py:265\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load LightGBM library.\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m find_lib_path()\n\u001b[0;32m--> 265\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m lib\u001b[38;5;241m.\u001b[39mLGBM_GetLastError\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[1;32m    267\u001b[0m callback \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCFUNCTYPE(\u001b[38;5;28;01mNone\u001b[39;00m, ctypes\u001b[38;5;241m.\u001b[39mc_char_p)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ctypes/__init__.py:452\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <231012EE-C624-39BF-9168-0899EB683529> /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import OptimalK\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import re\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "'''\n",
    "TODO: Visualize which MDS are most likely to be mixed up\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/data.csv'\n",
    "df = pd.read_csv(path, encoding='ISO-8859-1') # Using a different encoding while info is fixed\n",
    "df_win = pd.read_csv(path, encoding='Windows-1252') # For experimental purposes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set a path for the data that we want to use.\n",
    "* We utilize pandas to read the csv file and use a specified encoding.\n",
    "* This encoding ISO-8859-1 is also known as Latin-1 which is a character encoding standard for Western European Languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoder allows us to understand the csv file by converting the bytes into readable characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open('data/data.csv', 'rb') as f:\n",
    "    # Detect encoding\n",
    "    result = chardet.detect(f.read())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run chardet, which is a library that helps detect a file's encoding. \n",
    "* It outputs the recommended encoder along with the conifdence\n",
    "\n",
    "We have the correct encoder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Embeddings Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCategoriesSum(df, category_name: str):\n",
    "    category = dict()\n",
    "\n",
    "    for entry in df[category_name]:\n",
    "        if pd.isnull(entry):\n",
    "            continue  \n",
    "\n",
    "        items = entry.split(',')\n",
    "        for item in items:\n",
    "            item = item.strip() \n",
    "            if item:  \n",
    "                if item in category:\n",
    "                    category[item] += 1\n",
    "                else:\n",
    "                    category[item] = 1\n",
    "                    \n",
    "    return category\n",
    "\n",
    "\n",
    "def EmbedFrame(df, category_name: str, model: SentenceTransformer):\n",
    "    embeddings = model.encode(df[category_name].tolist())\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True) \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def CreateCluster(df, embeddings, num_clusters, title):\n",
    "\n",
    "    clustering_model = KMeans(n_clusters=num_clusters)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_assignments = clustering_model.labels_\n",
    "\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignments):\n",
    "        df.loc[sentence_id, title] = cluster_id\n",
    "\n",
    "\n",
    "def PrintCluster(cluster):\n",
    "    for cluster_id, mds_list in cluster.items():\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        print(\"-\" * 20)\n",
    "        for mds in mds_list:\n",
    "            print(f\"- {mds}\")\n",
    "\n",
    "def ReduceEmbedding(embedded_text, neighbors=100, min_distance=0.0):\n",
    "    reducer = umap.UMAP(n_neighbors=neighbors, n_components=2, random_state=42, min_dist=min_distance)\n",
    "    return reducer.fit_transform(embedded_text)\n",
    "    \n",
    "def VisualizeCluster(df, graph_title, reduced_embeddings, cluster_title: str, category: str = \"MDS Simplified\", show_authors=False, show_country=False, show_sample_size=False, show_patient_count=False, show_control_count=False, show_year=False, show_data_source=False, show_algorithm=False, show_task=False, show_cv=False, show_performance_metric=False, show_dataset=False, show_goal=False, show_wearable=False):\n",
    "    df[category] = df[category].str.strip().str.upper()\n",
    "    #Authors\tYear\tPaper\tCountry\tMDS\tData Source Primary\tData Source Secondary\tAlgorithm\tTask\tCross Validation\tPerformance Metric\tSample Size\tDataset\tGoal\tAbstract\tPerson\tNotes\tWearable Location\n",
    "    plot_df = pd.DataFrame({\n",
    "        'UMAP1': reduced_embeddings[:, 0],\n",
    "        'UMAP2': reduced_embeddings[:, 1],\n",
    "        'MDS Simplified': df[category],\n",
    "        'Paper': df['Paper'], \n",
    "        'MDS': df['MDS'],\n",
    "        cluster_title: df[cluster_title].astype(str),\n",
    "        \"Authors\": df[\"Authors\"],\n",
    "        \"Country\": df[\"Country\"],\n",
    "        \"Sample Size\": df[\"Sample Size\"],\n",
    "        \"Patient Count\": df[\"Patient Count\"],\n",
    "        \"Control Count\": df[\"Control Count\"],\n",
    "        \"Year\": df[\"Year\"],\n",
    "        \"Data Source Secondary\": df[\"Data Source Secondary\"],\n",
    "        \"Algorithm\" : df[\"Algorithm\"],\n",
    "        \"Task\": df[\"Task\"],\n",
    "        \"CV\" : df[\"Cross Validation\"],\n",
    "        \"Performance Metric\": df[\"Performance Metric\"],\n",
    "        \"Dataset\": df[\"Dataset\"],\n",
    "        \"Goal\": df[\"Goal\"],\n",
    "        \"Wearable\": df[\"Wearable Location\"]\n",
    "\n",
    "\n",
    "    })\n",
    "    \n",
    "    symbol_map = {\n",
    "        '0.0': 'circle',\n",
    "        '1.0': 'square',\n",
    "        '2.0': 'diamond',\n",
    "        '3.0': 'triangle-up',\n",
    "        '4.0': 'star',\n",
    "        '5.0': 'bowtie',\n",
    "        '6.0': 'pentagon',\n",
    "        '7.0': 'triangle-left',\n",
    "        '8.0': 'diamond-cross',\n",
    "        '9.0': 'triangle-down',\n",
    "        '10.0': 'triangle-right',\n",
    "    }\n",
    "\n",
    "    fig = px.scatter(\n",
    "        plot_df, \n",
    "        hover_data={'MDS': True, 'Paper': True, \"Authors\" : show_authors, \"Country\": show_country, \"Sample Size\": show_sample_size, \"Patient Count\": show_patient_count, \"Control Count\": show_control_count, \"Year\": show_year, \"Data Source Secondary\": show_data_source, \"Algorithm\": show_algorithm, \"Task\": show_task, \"CV\": show_cv, \"Performance Metric\": show_performance_metric, \"Dataset\": show_dataset, \"Goal\": show_goal, \"Wearable\": show_wearable}, \n",
    "        x='UMAP1', \n",
    "        y='UMAP2', \n",
    "        color='MDS Simplified', \n",
    "        symbol=cluster_title,   \n",
    "        symbol_map=symbol_map,   \n",
    "        title=graph_title,\n",
    "        labels={\"UMAP1\": \"UMAP Component 1\", \"UMAP2\": \"UMAP Component 2\"}\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        legend_title_text='MDS Simplified and Cluster ID',\n",
    "        title_font_size=16,\n",
    "        xaxis_title=\"UMAP Component 1\",\n",
    "        yaxis_title=\"UMAP Component 2\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def GetClusterAccuracy(df, cluster_title):\n",
    "    total_count = len(df[cluster_title])\n",
    "    mode_count = df[cluster_title].value_counts().max()\n",
    "    cluster_accuracy = mode_count / total_count\n",
    "    return cluster_accuracy\n",
    "\n",
    "def FormatSampleSize(df):\n",
    "    patient_list = []\n",
    "    control_list = []\n",
    "\n",
    "    for ss in df[\"Sample Size\"]:\n",
    "        if pd.isnull(ss) or \"NA\" in str(ss).upper():\n",
    "            patient_list.append(0)\n",
    "            control_list.append(0)\n",
    "            continue\n",
    "        \n",
    "        ss = str(ss)\n",
    "        patient_count = 0\n",
    "        control_count = 0\n",
    "\n",
    "        matches = re.findall(r'(\\d+)\\s*([A-Za-z]*)', ss)\n",
    "        for num_str, word in matches:\n",
    "            num = int(num_str)\n",
    "            if 'CONTROL' in word.upper():\n",
    "                control_count += num\n",
    "            else:\n",
    "                patient_count += num\n",
    "\n",
    "        patient_list.append(patient_count)\n",
    "        control_list.append(control_count)\n",
    "    \n",
    "    df[\"Patient Count\"] = patient_list\n",
    "    df[\"Control Count\"] = control_list\n",
    "\n",
    "\n",
    "def SimplifyMDS(df):\n",
    "    simplified_list = []\n",
    "    for mds in df[\"MDS\"]:\n",
    "        if pd.isnull(mds):\n",
    "            simplified_list.append(None)\n",
    "            continue\n",
    "        mds_upper = mds.strip().upper()\n",
    "        if \"ATAXIA\" in mds_upper:\n",
    "            simplified_list.append(\"ATAXIA\")\n",
    "        elif \"DYSTONIA\" in mds_upper:\n",
    "            simplified_list.append(\"DYSTONIA\")\n",
    "        elif \"BRADYKINESIA\" in mds_upper:\n",
    "            simplified_list.append(\"BRADYKINESIA\")\n",
    "        elif \"TREMOR\" in mds_upper:\n",
    "            simplified_list.append(\"TREMOR\")\n",
    "        elif \"ET\" in mds_upper:\n",
    "            simplified_list.append(\"ET\")\n",
    "        elif \"FOG\" in mds_upper:\n",
    "            simplified_list.append(\"FOG\")\n",
    "        elif \"DYSKINESIA\" in mds_upper:\n",
    "            simplified_list.append(\"DYSKINESIA\")\n",
    "        else:\n",
    "            simplified_list.append(mds_upper)\n",
    "\n",
    "    df[\"MDS Simplified\"] = simplified_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_whitespace(cell):\n",
    "    if isinstance(cell, str):  # Check if the value is a string ( if x is str)\n",
    "        return cell.strip()    # Strip whitespace from strings (leading and trailing)\n",
    "    return cell                # Return non-strings unchanged\n",
    "\n",
    "df = df.applymap(strip_whitespace)\n",
    "df_win = df_win.applymap(strip_whitespace)\n",
    "df = df.dropna(subset=[\"Authors\"]).reset_index(drop=True)\n",
    "df_win = df_win.dropna(subset=[\"Authors\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "SimplifyMDS(df)\n",
    "FormatSampleSize(df)\n",
    "SimplifyMDS(df_win)\n",
    "FormatSampleSize(df_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Change model to bio\n",
    "#model2 = SentenceTransformer(\"../models/sentence-transformers/emilyalsentzer_Bio_ClinicalBERTo_ClinicalBERT\")\n",
    "#model2 = SentenceTransformer(\"monologg/biobert_v1.1_pubmed\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "#model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "model = SentenceTransformer(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "'''\n",
    "- UMAP > t-SNE (better at preserving global structure & performance)\n",
    "    - Higher n_neighbor --> more global structure\n",
    "        - Higher noise, focus on large-scale trends\n",
    "    - Lower min_dist --> tightly packed, highly distinguishable clusters, emphasizing local density.\n",
    "        - Good for high-dimensional datasets where clusters are more continuous or overlapping.\n",
    "        - Suitable when your clusters are discrete and well-separated in high-dimensional space.\n",
    "        - Helpful for applications like classification tasks, where precise boundaries between clusters are desired.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GENERAL VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['Country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.tab20c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Papers by country**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the horizontal bar graph with spacing between bars\n",
    "country_counts = df['Country'].value_counts()\n",
    "print(\"Amount of Countries:\", len(country_counts))  # Prints the length of the list\n",
    "paper_counts = df['Paper'].value_counts()\n",
    "print(\"Amount of Papers:\", len(paper_counts))  # Prints the length of the list\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(country_counts.index, country_counts.values, color=cm.tab20c.colors[:len(country_counts)])\n",
    "# Set the x-axis label\n",
    "max_count = country_counts.max()\n",
    "plt.xlim(0, max_count + max_count * 0.1)\n",
    "plt.margins(y=0.005)\n",
    "# Labels and title with padding\n",
    "plt.xlabel('Number of Papers')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Number of Papers by Country')\n",
    "\n",
    "for i, v in enumerate(country_counts.values):\n",
    "        plt.text(v + 1, i, str(v))\n",
    "\n",
    "# Increase spacing between plot elements if needed\n",
    "plt.subplots_adjust(left=0.3, right=0.9, top=1.5, bottom=0.05)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Papers by Person**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the horizontal bar graph with spacing between bars\n",
    "person_counts = df['Person'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(person_counts.index, person_counts.values, color=cm.tab20.colors[:len(person_counts)])\n",
    "# Set the x-axis label\n",
    "max_count = person_counts.max()\n",
    "plt.xlim(0, max_count + max_count * 0.1)\n",
    "plt.margins(y=0.005)\n",
    "# Labels and title with padding\n",
    "plt.xlabel('Number of Papers')\n",
    "plt.ylabel('Person')\n",
    "plt.title('Number of Papers per Person')\n",
    "\n",
    "# for i, v in enumerate(person_counts.values):\n",
    "#         plt.text(v + 1, i, str(v))\n",
    "\n",
    "# Increase spacing between plot elements if needed\n",
    "plt.subplots_adjust(left=0.3, right=0.9, top=0.5, bottom=0.05)\n",
    "# plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "# Optional: invert y-axis for descending order\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source Counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = GetCategoriesSum(df, \"Data Source Secondary\")\n",
    "print(\"Amount of Data Source Secondary: \",len(category_counts))\n",
    "\n",
    "# Convert dictionary to Series for plotting\n",
    "category_counts_series = pd.Series(category_counts)\n",
    "category_counts_series = category_counts_series.sort_values()\n",
    "\n",
    "cmap = plt.get_cmap('terrain')  # Choose your colormap\n",
    "colors = cmap(np.linspace(0.1, 0.9, len(category_counts_series)))  # Adjust the range (e.g., 0.2 to 0.8 for less dominance of one color)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(10, 20))\n",
    "bars = plt.barh(category_counts_series.index, category_counts_series.values, color = colors)\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width() + 1,  # Position text slightly beyond the bar\n",
    "        bar.get_y() + bar.get_height() / 2,  # Center text vertically\n",
    "        f'{int(bar.get_width())}',  # Convert value to integer and format\n",
    "        va='center',  # Vertical alignment\n",
    "        fontsize=10,  # Font size for the text\n",
    "        color='black'  # Text color\n",
    "    )\n",
    "\n",
    "\n",
    "plt.xlabel('Categories', fontsize=14)\n",
    "plt.ylabel('Counts', fontsize=14)\n",
    "plt.title('Data Source Secondary Counts', fontsize=16)\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Top 20*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = GetCategoriesSum(df_win, \"Data Source Secondary\")\n",
    "print(\"Data Source Secondary Total:\", len(category_counts))\n",
    "\n",
    "# Convert dictionary to Series for plotting\n",
    "category_counts_series = pd.Series(category_counts)\n",
    "\n",
    "# Sort values in ascending order and select the top 20\n",
    "top_20 = category_counts_series.sort_values(ascending=False).head(20).sort_values()\n",
    "\n",
    "# Generate a gradient color for the top 20\n",
    "cmap = plt.get_cmap('terrain') \n",
    "colors = cmap(np.linspace(0.1, 0.9, len(top_20)))\n",
    "\n",
    "# Plotting the bar chart for the top 20\n",
    "plt.figure(figsize=(14, 17))  # Adjust figure size for fewer bars\n",
    "bars = plt.barh(top_20.index, top_20.values, color=colors)\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width() + 1,  # Position text slightly beyond the bar\n",
    "        bar.get_y() + bar.get_height() / 2,  # Center text vertically\n",
    "        f'{int(bar.get_width())}',  # Convert value to integer and format\n",
    "        va='center',  # Vertical alignment\n",
    "        fontsize=15,  # Font size for the text\n",
    "        color='black'  # Text color\n",
    "    )\n",
    "\n",
    "\n",
    "plt.xlabel('Counts', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Categories', fontsize=20, fontweight='bold')\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.title('Top 20 Data Source Secondary Count', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Adjust layout for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = GetCategoriesSum(df_win, \"Algorithm\")\n",
    "print(\"Amount of Algorithms:\", len(category_counts))  # Prints the length of the list\n",
    "# Convert dictionary to Series for plotting\n",
    "category_counts_series = pd.Series(category_counts)\n",
    "category_counts_series = category_counts_series.sort_values()\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('plasma')  # Choose your colormap\n",
    "colors = cmap(np.linspace(0.1, 0.9, len(category_counts_series)))  # Adjust the range (e.g., 0.2 to 0.8 for less dominance of one color)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(15, 35))\n",
    "plt.barh(category_counts_series.index, category_counts_series.values, color=colors)\n",
    "plt.xlabel('Categories', fontsize=14)\n",
    "plt.ylabel('Counts', fontsize=14)\n",
    "plt.title('ML Algorithm Count', fontsize=16)\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Top 20*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = GetCategoriesSum(df_win, \"Algorithm\")\n",
    "\n",
    "# Convert dictionary to Series for plotting\n",
    "category_counts_series = pd.Series(category_counts)\n",
    "\n",
    "# Sort values in ascending order and select the top 20\n",
    "top_20 = category_counts_series.sort_values(ascending=False).head(20).sort_values()\n",
    "\n",
    "# Generate a gradient color for the top 20\n",
    "cmap = plt.get_cmap('plasma') \n",
    "colors = cmap(np.linspace(0.1, 0.9, len(top_20)))\n",
    "\n",
    "# Plotting the bar chart for the top 20\n",
    "plt.figure(figsize=(10, 15))  # Adjust figure size for fewer bars\n",
    "bars = plt.barh(top_20.index, top_20.values, color=colors)\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width() + 1,  # Position text slightly beyond the bar\n",
    "        bar.get_y() + bar.get_height() / 2,  # Center text vertically\n",
    "        f'{int(bar.get_width())}',  # Convert value to integer and format\n",
    "        va='center',  # Vertical alignment\n",
    "        fontsize=15,  # Font size for the text\n",
    "        color='black'  # Text color\n",
    "    )\n",
    "\n",
    "plt.xlabel('Counts', fontsize=20, fontweight ='bold')\n",
    "plt.ylabel('Categories', fontsize=20, fontweight ='bold')\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.title('Top 20 ML Algorithm Count', fontsize=20, fontweight ='bold')\n",
    "\n",
    "# Adjust layout for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = GetCategoriesSum(df_win, \"MDS\")\n",
    "print(\"Amount of MDS:\", len(category_counts))  # Prints the length of the list\n",
    "# Convert dictionary to Series for plotting\n",
    "category_counts_series = pd.Series(category_counts)\n",
    "category_counts_series = category_counts_series.sort_values()\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm')  # Choose your colormap\n",
    "colors = cmap(np.linspace(0.1, 0.9, len(category_counts_series)))  # Adjust the range (e.g., 0.2 to 0.8 for less dominance of one color)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(15, 15))\n",
    "bars = plt.barh(category_counts_series.index, category_counts_series.values, color=colors)\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width() + 1,  # Position text slightly beyond the bar\n",
    "        bar.get_y() + bar.get_height() / 2,  # Center text vertically\n",
    "        f'{int(bar.get_width())}',  # Convert value to integer and format\n",
    "        va='center',  # Vertical alignment\n",
    "        fontsize=15,  # Font size for the text\n",
    "        color='black'  # Text color\n",
    "    )\n",
    "\n",
    "plt.xlabel('Categories', fontsize=24, fontweight = 'bold')\n",
    "plt.xticks(fontsize=15,rotation=30)\n",
    "plt.ylabel('Counts', fontsize=28, fontweight = 'bold')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title('MDS Count', fontsize=28, fontweight = 'bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF encoder difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes- we can see if there is any major or slight differences by choosing a different encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PAPER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Paper\"] = df[\"Paper\"].fillna(\"\")\n",
    "embedded_papers_original = EmbedFrame(df, \"Paper\", model)\n",
    "embedded_reduced_papers_original = ReduceEmbedding(embedded_papers_original, neighbors=100, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_papers_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_papers_original, num_clusters, \"Reduced Paper Cluster\")\n",
    "VisualizeCluster(df, \"Title\", embedded_reduced_papers_original, \"Reduced Paper Cluster\")\n",
    "\n",
    "print(\"Toal clusters: \",num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Paper\"] = df_win[\"Paper\"].fillna(\"\")\n",
    "embedded_papers_win = EmbedFrame(df_win, \"Paper\", model)\n",
    "embedded_reduced_papers_win = ReduceEmbedding(embedded_papers_win, neighbors=100, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_papers_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_papers_win, num_clusters, \"Reduced Paper Cluster\")\n",
    "VisualizeCluster(df_win, \"Title\", embedded_reduced_papers_win, \"Reduced Paper Cluster\")\n",
    "\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AUTHORS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Authors\"] = df[\"Authors\"].fillna(\"\")\n",
    "embedded_authors_original = EmbedFrame(df, \"Authors\", model)\n",
    "embedded_reduced_authors_original = ReduceEmbedding(embedded_authors_original, neighbors=2, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_authors_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "print(\"Toal clusters: \",num_clusters)\n",
    "CreateCluster(df, embedded_reduced_authors_original, num_clusters, \"Reduced Authors Cluster\")\n",
    "VisualizeCluster(df, \"Authors\", embedded_reduced_authors_original, \"Reduced Authors Cluster\", show_authors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Authors\"] = df_win[\"Authors\"].fillna(\"\")\n",
    "embedded_authors_win = EmbedFrame(df_win, \"Authors\", model)\n",
    "embedded_reduced_authors_win = ReduceEmbedding(embedded_authors_win, neighbors=2, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_authors_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_authors_win, num_clusters, \"Reduced Authors Cluster\")\n",
    "VisualizeCluster(df_win, \"Authors\", embedded_reduced_authors_win, \"Reduced Authors Cluster\", show_authors=True)\n",
    "\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ABSTRACT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Abstract\"] = df[\"Abstract\"].fillna(\"\")\n",
    "embedded_abstracts_original = EmbedFrame(df, \"Abstract\", model)\n",
    "embedded_reduced_abstracts_original = ReduceEmbedding(embedded_abstracts_original, neighbors=50, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_abstracts_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_abstracts_original, num_clusters, \"Reduced Abstract Cluster\")\n",
    "VisualizeCluster(df, \"Abstract\", embedded_reduced_abstracts_original, \"Reduced Abstract Cluster\")\n",
    "\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Abstract\"] = df_win[\"Abstract\"].fillna(\"\")\n",
    "embedded_abstracts_win = EmbedFrame(df_win, \"Abstract\", model)\n",
    "embedded_reduced_abstracts_win = ReduceEmbedding(embedded_abstracts_win, neighbors=50, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_abstracts_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_abstracts_win, num_clusters, \"Reduced Abstract Cluster\")\n",
    "VisualizeCluster(df_win, \"Abstract\", embedded_reduced_abstracts_win, \"Reduced Abstract Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COUNTRY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Country\"] = df[\"Country\"].fillna(\"\")\n",
    "embedded_countries_original = EmbedFrame(df, \"Country\", model)\n",
    "embedded_reduced_countries_original = ReduceEmbedding(embedded_countries_original, neighbors=12, min_distance=.25)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_countries_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_countries_original, num_clusters, \"Reduced Country Cluster\")\n",
    "VisualizeCluster(df, \"Country\", embedded_reduced_countries_original, \"Reduced Country Cluster\", show_country=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Country\"] = df[\"Country\"].fillna(\"\")\n",
    "embedded_countries_original = EmbedFrame(df, \"Country\", model)\n",
    "embedded_reduced_countries_original = ReduceEmbedding(embedded_countries_original, neighbors=12, min_distance=.25)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_countries_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_countries_original, num_clusters, \"Reduced Country Cluster\")\n",
    "VisualizeCluster(df, \"Country\", embedded_reduced_countries_original, \"Reduced Country Cluster\", show_country=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Country\"] = df_win[\"Country\"].fillna(\"\")\n",
    "embedded_countries_win = EmbedFrame(df_win, \"Country\", model)\n",
    "embedded_reduced_countries_win = ReduceEmbedding(embedded_countries_win, neighbors=12, min_distance=.25)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_countries_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_countries_win, num_clusters, \"Reduced Country Cluster\")\n",
    "VisualizeCluster(df_win, \"Country\", embedded_reduced_countries_win, \"Reduced Country Cluster\", show_country=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA SOURCE SECONDARY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Data Source Secondary\"] = df[\"Data Source Secondary\"].fillna(\"\")\n",
    "embedded_dss_original = EmbedFrame(df, \"Data Source Secondary\", model)\n",
    "embedded_reduced_dss_original = ReduceEmbedding(embedded_dss_original, neighbors=10, min_distance=.45)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_dss_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_dss_original, num_clusters, \"Reduced Data Source Secondary Cluster\")\n",
    "VisualizeCluster(df, \"Data Source Secondary\", embedded_reduced_dss_original, \"Reduced Data Source Secondary Cluster\", show_data_source=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Data Source Secondary\"] = df_win[\"Data Source Secondary\"].fillna(\"\")\n",
    "embedded_dss_win = EmbedFrame(df_win, \"Data Source Secondary\", model)\n",
    "embedded_reduced_dss_win = ReduceEmbedding(embedded_dss_win, neighbors=10, min_distance=.45)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_dss_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_dss_win, num_clusters, \"Reduced Data Source Secondary Cluster\")\n",
    "VisualizeCluster(df_win, \"Data Source Secondary\", embedded_reduced_dss_win, \"Reduced Data Source Secondary Cluster\", show_data_source=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ALGORITHM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Algorithm\"] = df[\"Algorithm\"].fillna(\"\")\n",
    "embedded_algorithms_original = EmbedFrame(df, \"Algorithm\", model)\n",
    "embedded_reduced_algorithms_original = ReduceEmbedding(embedded_algorithms_original, neighbors=10, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_algorithms_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_algorithms_original, num_clusters, \"Reduced Algorithm Cluster\")\n",
    "VisualizeCluster(df, \"Algorithm\", embedded_reduced_algorithms_original, \"Reduced Algorithm Cluster\", show_algorithm=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Algorithm\"] = df_win[\"Algorithm\"].fillna(\"\")\n",
    "embedded_algorithms_win = EmbedFrame(df_win, \"Algorithm\", model)\n",
    "embedded_reduced_algorithms_win = ReduceEmbedding(embedded_algorithms_win, neighbors=10, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_algorithms_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_algorithms_win, num_clusters, \"Reduced Algorithm Cluster\")\n",
    "VisualizeCluster(df_win, \"Algorithm\", embedded_reduced_algorithms_win, \"Reduced Algorithm Cluster\", show_algorithm=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TASK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Task\"] = df[\"Task\"].fillna(\"\")\n",
    "embedded_tasks_original = EmbedFrame(df, \"Task\", model)\n",
    "embedded_reduced_tasks_original = ReduceEmbedding(embedded_tasks_original, neighbors=5, min_distance=.35)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_tasks_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_tasks_original, num_clusters, \"Reduced Task Cluster\")\n",
    "VisualizeCluster(df, \"Task\", embedded_reduced_tasks_original, \"Reduced Task Cluster\", show_task=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Task\"] = df_win[\"Task\"].fillna(\"\")\n",
    "embedded_tasks_win = EmbedFrame(df_win, \"Task\", model)\n",
    "embedded_reduced_tasks_win = ReduceEmbedding(embedded_tasks_win, neighbors=5, min_distance=.35)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_tasks_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_tasks_win, num_clusters, \"Reduced Task Cluster\")\n",
    "VisualizeCluster(df_win, \"Task\", embedded_reduced_tasks_win, \"Reduced Task Cluster\", show_task=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CROSS VALIDATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cross Validation\"] = df[\"Cross Validation\"].fillna(\"\")\n",
    "embedded_cvs_original = EmbedFrame(df, \"Cross Validation\", model)\n",
    "embedded_reduced_cvs_original = ReduceEmbedding(embedded_cvs_original, neighbors=500, min_distance=.25)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_cvs_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_cvs_original, num_clusters, \"Reduced Cross Validation Cluster\")\n",
    "VisualizeCluster(df, \"Cross Validation\", embedded_reduced_cvs_original, \"Reduced Cross Validation Cluster\", show_cv=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Cross Validation\"] = df_win[\"Cross Validation\"].fillna(\"\")\n",
    "embedded_cvs_win = EmbedFrame(df_win, \"Cross Validation\", model)\n",
    "embedded_reduced_cvs_win = ReduceEmbedding(embedded_cvs_win, neighbors=500, min_distance=.25)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_cvs_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_cvs_win, num_clusters, \"Reduced Cross Validation Cluster\")\n",
    "VisualizeCluster(df_win, \"Cross Validation\", embedded_reduced_cvs_win, \"Reduced Cross Validation Cluster\", show_cv=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PERFORMANCE METRIC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Performance Metric\"] = df[\"Performance Metric\"].fillna(\"\")\n",
    "embedded_pms_original = EmbedFrame(df, \"Performance Metric\", model)\n",
    "embedded_reduced_pms_original = ReduceEmbedding(embedded_pms_original, neighbors=50, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_pms_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_pms_original, num_clusters, \"Reduced Performance Metric Cluster\")\n",
    "VisualizeCluster(df, \"Performance Metric\", embedded_reduced_pms_original, \"Reduced Performance Metric Cluster\", show_performance_metric=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Performance Metric\"] = df_win[\"Performance Metric\"].fillna(\"\")\n",
    "embedded_pms_win = EmbedFrame(df_win, \"Performance Metric\", model)\n",
    "embedded_reduced_pms_win = ReduceEmbedding(embedded_pms_win, neighbors=50, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_pms_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_pms_win, num_clusters, \"Reduced Performance Metric Cluster\")\n",
    "VisualizeCluster(df_win, \"Performance Metric\", embedded_reduced_pms_win, \"Reduced Performance Metric Cluster\", show_performance_metric=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TODO: Use Scatter Plot w/ Integers\n",
    "df[\"Sample Size\"] = df[\"Sample Size\"].fillna(\"\")\n",
    "\n",
    "embedded_patient_count = EmbedFrame(df, \"Patient Count\", model)\n",
    "embedded_control_count = EmbedFrame(df, \"Control Count\", model)\n",
    "\n",
    "embedded_reduced_patient_count = ReduceEmbedding(embedded_patient_count, neighbors=50, min_distance=0)\n",
    "embedded_reduced_control_count = ReduceEmbedding(embedded_control_count, neighbors=50, min_distance=0)\n",
    "\n",
    "embedded_sample_size = np.concatenate((embedded_reduced_patient_count, embedded_reduced_control_count), axis=1)\n",
    "\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_sample_size, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_sample_size, num_clusters, \"Reduced Patient Count Cluster\")\n",
    "VisualizeCluster(df, \"Patient Count\", embedded_sample_size, \"Reduced Patient Count Cluster\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write out full dataset acronym\n",
    "df[\"Dataset\"] = df[\"Dataset\"].fillna(\"\")\n",
    "embedded_datasets_original = EmbedFrame(df, \"Dataset\", model)\n",
    "embedded_reduced_datasets_original = ReduceEmbedding(embedded_datasets_original, neighbors=2, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_datasets_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_datasets_original, num_clusters, \"Reduced Dataset Cluster\")\n",
    "VisualizeCluster(df, \"Dataset\", embedded_reduced_datasets_original, \"Reduced Dataset Cluster\", show_dataset=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write out full dataset acronym\n",
    "df_win[\"Dataset\"] = df_win[\"Dataset\"].fillna(\"\")\n",
    "embedded_datasets_win = EmbedFrame(df_win, \"Dataset\", model)\n",
    "embedded_reduced_datasets_win = ReduceEmbedding(embedded_datasets_win, neighbors=2, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_datasets_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_datasets_win, num_clusters, \"Reduced Dataset Cluster\")\n",
    "VisualizeCluster(df_win, \"Dataset\", embedded_reduced_datasets_win, \"Reduced Dataset Cluster\", show_dataset=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GOAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Goal\"] = df[\"Goal\"].fillna(\"\")\n",
    "embedded_goals_original = EmbedFrame(df, \"Goal\", model)\n",
    "embedded_reduced_goals_original = ReduceEmbedding(embedded_goals_original, neighbors=2, min_distance=.5)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_goals_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_goals_original, num_clusters, \"Reduced Goal Cluster\")\n",
    "VisualizeCluster(df, \"Goal\", embedded_reduced_goals_original, \"Reduced Goal Cluster\", show_goal=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Goal\"] = df_win[\"Goal\"].fillna(\"\")\n",
    "embedded_goals_win = EmbedFrame(df_win, \"Goal\", model)\n",
    "embedded_reduced_goals_win = ReduceEmbedding(embedded_goals_win, neighbors=2, min_distance=.5)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_goals_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_goals_win, num_clusters, \"Reduced Goal Cluster\")\n",
    "VisualizeCluster(df_win, \"Goal\", embedded_reduced_goals_win, \"Reduced Goal Cluster\", show_goal=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WEARABLE LOCATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Wearable Location\"] = df[\"Wearable Location\"].fillna(\"\")\n",
    "embedded_wearable_locs_original = EmbedFrame(df, \"Wearable Location\", model)\n",
    "embedded_reduced_wearable_locs_original = ReduceEmbedding(embedded_wearable_locs_original, neighbors=3, min_distance=1)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_wearable_locs_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_wearable_locs_original, num_clusters, \"Reduced Wearable Location Cluster\")\n",
    "VisualizeCluster(df, \"Wearable Location\", embedded_reduced_wearable_locs_original, \"Reduced Wearable Location Cluster\", show_wearable=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_win[\"Wearable Location\"] = df_win[\"Wearable Location\"].fillna(\"\")\n",
    "embedded_wearable_locs_win = EmbedFrame(df_win, \"Wearable Location\", model)\n",
    "embedded_reduced_wearable_locs_win = ReduceEmbedding(embedded_wearable_locs_win, neighbors=3, min_distance=1)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_wearable_locs_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_wearable_locs_win, num_clusters, \"Reduced Wearable Location Cluster\")\n",
    "VisualizeCluster(df_win, \"Wearable Location\", embedded_reduced_wearable_locs_win, \"Reduced Wearable Location Cluster\", show_wearable=True)\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HIGH DIMENSIONALITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # High Dimensionality, beneficial if each feature captures unique and important data, diluted distance\n",
    "\n",
    "embedding_list_win = [     \n",
    "    embedded_reduced_wearable_locs_win,\n",
    "    embedded_authors_win,\n",
    "    embedded_reduced_tasks_win,\n",
    "    embedded_reduced_datasets_win,\n",
    "    embedded_reduced_papers_win,\n",
    "    embedded_reduced_dss_win,\n",
    "    embedded_reduced_goals_win,\n",
    "    embedded_reduced_abstracts_win,\n",
    "    embedded_reduced_countries_win,\n",
    "    embedded_reduced_algorithms_win,\n",
    "    embedded_reduced_cvs_win,\n",
    "    embedded_reduced_pms_win\n",
    "]\n",
    "\n",
    "high_dimensionality_embeddings_win = np.concatenate(embedding_list_win, axis=1)\n",
    "\n",
    "embedded_reduced_high_dim_win = ReduceEmbedding(high_dimensionality_embeddings_win, neighbors=2, min_distance=0)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_high_dim_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_high_dim_win, num_clusters, \"Reduced High Dimensionality Total Cluster\")\n",
    "VisualizeCluster(df_win, \"Reduced High Dimensionality\", embedded_reduced_high_dim_win, \"Reduced High Dimensionality Total Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LOW DIMENSIONALITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use \"memory\" of previous clusters by column + all base data\n",
    "# Captures patterns throughout features, but may dilute the importance of individual features - distances are more meaningful - \n",
    "df[\"Low Dimensionality Total\"] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "df[\"Low Dimensionality Total\"] = df[\"Low Dimensionality Total\"].replace('nan', '')\n",
    "\n",
    "\n",
    "embedded_low_dimensionality_original = EmbedFrame(df, \"Low Dimensionality Total\", model)\n",
    "embedded_reduced_low_dimensionality_original = ReduceEmbedding(embedded_low_dimensionality_original, neighbors=3, min_distance=1)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_low_dimensionality_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_low_dimensionality_original, num_clusters, \"Reduced Low Dimensionality Total Cluster\")\n",
    "VisualizeCluster(df, \"Low Dimensionality Total\", embedded_reduced_low_dimensionality_original, \"Reduced Low Dimensionality Total Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use \"memory\" of previous clusters by column + all base data\n",
    "# Captures patterns throughout features, but may dilute the importance of individual features - distances are more meaningful - \n",
    "df_win[\"Low Dimensionality Total\"] = df_win.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "df_win[\"Low Dimensionality Total\"] = df_win[\"Low Dimensionality Total\"].replace('nan', '')\n",
    "\n",
    "\n",
    "embedded_low_dimensionality_win = EmbedFrame(df, \"Low Dimensionality Total\", model)\n",
    "embedded_reduced_low_dimensionality_win = ReduceEmbedding(embedded_low_dimensionality_win, neighbors=3, min_distance=1)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters = optimalKObj(embedded_reduced_low_dimensionality_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_low_dimensionality_win, num_clusters, \"Reduced Low Dimensionality Total Cluster\")\n",
    "VisualizeCluster(df_win, \"Low Dimensionality Total\", embedded_reduced_low_dimensionality_win, \"Reduced Low Dimensionality Total Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df = df.drop('Reduced Paper Cluster', axis=1)\n",
    "df = df.drop('Reduced Authors Cluster', axis=1)\n",
    "df = df.drop('Reduced Abstract Cluster', axis=1)\n",
    "df = df.drop('Reduced Country Cluster', axis=1)\n",
    "df = df.drop('Reduced Data Source Secondary Cluster', axis=1)\n",
    "df = df.drop('Reduced Algorithm Cluster', axis=1)\n",
    "df = df.drop('Reduced Task Cluster', axis=1)\n",
    "df = df.drop('Reduced Cross Validation Cluster', axis=1)\n",
    "df = df.drop('Reduced Performance Metric Cluster', axis=1)\n",
    "df = df.drop('Reduced Dataset Cluster', axis=1)\n",
    "df = df.drop('Reduced Goal Cluster', axis=1)\n",
    "df = df.drop('Reduced Wearable Location Cluster', axis=1)\n",
    "df = df.drop('Reduced High Dimensionality Total Cluster', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not have \"memory\" of previous clusters + all base data\n",
    "df[\"Low Dimensionality Total\"] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "df[\"Low Dimensionality Total\"] = df[\"Low Dimensionality Total\"].replace('nan', '')\n",
    "\n",
    "\n",
    "embedded_low_dimensionality_original = EmbedFrame(df, \"Low Dimensionality Total\", model)\n",
    "embedded_reduced_low_dimensionality_original = ReduceEmbedding(embedded_low_dimensionality_original, neighbors=3, min_distance=1)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters_original = optimalKObj(embedded_reduced_low_dimensionality_original, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df, embedded_reduced_low_dimensionality_original, num_clusters_original, \"Reduced Low Dimensionality Total Cluster\")\n",
    "VisualizeCluster(df, \"Low Dimensionality Total\", embedded_reduced_low_dimensionality_original, \"Reduced Low Dimensionality Total Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not have \"memory\" of previous clusters + all base data\n",
    "df_win[\"Low Dimensionality Total\"] = df_win.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "df_win[\"Low Dimensionality Total\"] = df_win[\"Low Dimensionality Total\"].replace('nan', '')\n",
    "\n",
    "\n",
    "embedded_low_dimensionality_win = EmbedFrame(df_win, \"Low Dimensionality Total\", model)\n",
    "embedded_reduced_low_dimensionality_win = ReduceEmbedding(embedded_low_dimensionality_win, neighbors=3, min_distance=1)\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters_win = optimalKObj(embedded_reduced_low_dimensionality_win, n_refs=50, cluster_array=np.arange(1,10))\n",
    "CreateCluster(df_win, embedded_reduced_low_dimensionality_win, num_clusters_win, \"Reduced Low Dimensionality Total Cluster\")\n",
    "VisualizeCluster(df_win, \"Low Dimensionality Total\", embedded_reduced_low_dimensionality_win, \"Reduced Low Dimensionality Total Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ISO encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Review Above Graphs for most consequential/separate clusters and adjust weights accordingly\n",
    "weights = [\n",
    "    0.0,  # Weight for embedded_authors\n",
    "    0.0,  # Weight for embedded_reduced_wearable_locs\n",
    "    0.0,  # Weight for embedded_reduced_tasks\n",
    "    0.0,  # Weight for embedded_reduced_datasets\n",
    "    0.0,  # Weight for embedded_reduced_papers\n",
    "    0.0,  # Weight for embedded_reduced_dss\n",
    "    0.0,  # Weight for embedded_reduced_goals\n",
    "    1.0,  # Weight for embedded_reduced_abstracts\n",
    "    0.0,  # Weight for embedded_reduced_countries\n",
    "    0.0,  # Weight for embedded_reduced_algorithms\n",
    "    0.0,  # Weight for embedded_reduced_cvs\n",
    "    0.0,  # Weight for embedded_reduced_pms\n",
    "]\n",
    "\n",
    "embedding_reduced_list_original = [\n",
    "    embedded_authors_original,\n",
    "    embedded_reduced_wearable_locs_original,\n",
    "    embedded_reduced_tasks_original,\n",
    "    embedded_reduced_datasets_original,\n",
    "    embedded_reduced_papers_original,\n",
    "    embedded_reduced_dss_original,\n",
    "    embedded_reduced_goals_original,\n",
    "    embedded_reduced_abstracts_original,\n",
    "    embedded_reduced_countries_original,\n",
    "    embedded_reduced_algorithms_original,\n",
    "    embedded_reduced_cvs_original,\n",
    "    embedded_reduced_pms_original\n",
    "]\n",
    "\n",
    "weighted_embeddings_original = [embedding * weight for embedding, weight in zip(embedding_reduced_list_original, weights)]\n",
    "\n",
    "weighted_embeddings_original = [embedding for embedding in weighted_embeddings_original if not np.all(embedding == 0)]\n",
    "\n",
    "if len(weighted_embeddings_original) > 1:\n",
    "    high_dimensionality_embeddings_original = np.concatenate(weighted_embeddings_original, axis=1)\n",
    "elif len(weighted_embeddings_original) == 1:\n",
    "    high_dimensionality_embeddings_original = weighted_embeddings_original[0]\n",
    "else:\n",
    "    raise ValueError(\"All weights are zero. Please assign a non-zero weight to at least one embedding.\")\n",
    "\n",
    "embedded_reduced_high_dim_original = ReduceEmbedding(high_dimensionality_embeddings_original, neighbors=50, min_distance=0)\n",
    "\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters_original = optimalKObj(embedded_reduced_high_dim_original, n_refs=50, cluster_array=np.arange(1, 10))\n",
    "\n",
    "CreateCluster(df, embedded_reduced_high_dim_original, num_clusters_original, \"Adjusted Weighted Cluster\")\n",
    "VisualizeCluster(df, \"Adjusted Weighted Embeddings\", embedded_reduced_high_dim_original, \"Adjusted Weighted Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Review Above Graphs for most consequential/separate clusters and adjust weights accordingly\n",
    "weights = [\n",
    "    0.0,  # Weight for embedded_authors\n",
    "    0.0,  # Weight for embedded_reduced_wearable_locs\n",
    "    0.0,  # Weight for embedded_reduced_tasks\n",
    "    0.0,  # Weight for embedded_reduced_datasets\n",
    "    0.0,  # Weight for embedded_reduced_papers\n",
    "    0.0,  # Weight for embedded_reduced_dss\n",
    "    0.0,  # Weight for embedded_reduced_goals\n",
    "    1.0,  # Weight for embedded_reduced_abstracts\n",
    "    0.0,  # Weight for embedded_reduced_countries\n",
    "    0.0,  # Weight for embedded_reduced_algorithms\n",
    "    0.0,  # Weight for embedded_reduced_cvs\n",
    "    0.0,  # Weight for embedded_reduced_pms\n",
    "]\n",
    "\n",
    "embedding_reduced_list_win = [\n",
    "    embedded_authors_win,\n",
    "    embedded_reduced_wearable_locs_win,\n",
    "    embedded_reduced_tasks_win,\n",
    "    embedded_reduced_datasets_win,\n",
    "    embedded_reduced_papers_win,\n",
    "    embedded_reduced_dss_win,\n",
    "    embedded_reduced_goals_win,\n",
    "    embedded_reduced_abstracts_win,\n",
    "    embedded_reduced_countries_win,\n",
    "    embedded_reduced_algorithms_win,\n",
    "    embedded_reduced_cvs_win,\n",
    "    embedded_reduced_pms_win,\n",
    "]\n",
    "\n",
    "weighted_embeddings_win = [embedding * weight for embedding, weight in zip(embedding_reduced_list_win, weights)]\n",
    "\n",
    "weighted_embeddings_win = [embedding for embedding in weighted_embeddings_win if not np.all(embedding == 0)]\n",
    "\n",
    "if len(weighted_embeddings_win) > 1:\n",
    "    high_dimensionality_embeddings_win = np.concatenate(weighted_embeddings_win, axis=1)\n",
    "elif len(weighted_embeddings_win) == 1:\n",
    "    high_dimensionality_embeddings_win = weighted_embeddings_win[0]\n",
    "else:\n",
    "    raise ValueError(\"All weights are zero. Please assign a non-zero weight to at least one embedding.\")\n",
    "\n",
    "embedded_reduced_high_dim_win = ReduceEmbedding(high_dimensionality_embeddings_win, neighbors=50, min_distance=0)\n",
    "\n",
    "optimalKObj = OptimalK.OptimalK(n_jobs=4, parallel_backend='joblib')\n",
    "num_clusters_win = optimalKObj(embedded_reduced_high_dim_win, n_refs=50, cluster_array=np.arange(1, 10))\n",
    "\n",
    "CreateCluster(df_win, embedded_reduced_high_dim_win, num_clusters_win, \"Adjusted Weighted Cluster\")\n",
    "VisualizeCluster(df_win, \"Adjusted Weighted Embeddings\", embedded_reduced_high_dim_win, \"Adjusted Weighted Cluster\")\n",
    "print(\"Toal clusters: \",num_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
